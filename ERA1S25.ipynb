{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "# Twin-Delayed DDPG\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "HAHMB0Ze8fU0",
        "outputId": "9fcce421-5e7d-49c4-e451-cf87d639b797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "  Using cached pybullet-3.2.6.tar.gz (80.5 MB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: pybullet\n",
            "  Building wheel for pybullet (setup.py): started\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): still running...\n",
            "  Building wheel for pybullet (setup.py): finished with status 'done'\n",
            "  Created wheel for pybullet: filename=pybullet-3.2.6-cp310-cp310-win_amd64.whl size=67441314 sha256=5a70f5c2c1642f5e74d9d8a681da8e2b04430fb5fdf319aeae00a52444f1bd28\n",
            "  Stored in directory: c:\\users\\navya\\appdata\\local\\pip\\cache\\wheels\\52\\17\\85\\7ebb8c1f1c59ed08fc2d259f51201cfa5849368ed5e421f83d\n",
            "Successfully built pybullet\n",
            "Installing collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 23.3.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# ! pip install gym==0.23.1\n",
        "# ! pip install --upgrade --force-reinstall pybullet==3.2.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ikr2p0Js8iB4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u5rW0IDB8nTO"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind:\n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4CeRW4D79HL0"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OCee7gwR9Jrs"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zzd0H1xukdKe"
      },
      "outputs": [],
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "\n",
        "    for it in range(iterations):\n",
        "\n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "\n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qabqiYdp9wDM"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HFj6wbAo97lk"
      },
      "outputs": [],
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "1fyH8N5z-o3o",
        "outputId": "103bebbe-16c8-44df-af22-49772fe1f427"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Src07lvY-zXb"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "CyQXJUIs-6BV",
        "outputId": "0069b80e-e1f5-432b-88d9-6260e05757f6"
      },
      "outputs": [],
      "source": [
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z3RufYec_ADj"
      },
      "outputs": [],
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wTVvG7F8_EWg"
      },
      "outputs": [],
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "[link text](https://)## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sd-ZsdXR_LgV"
      },
      "outputs": [],
      "source": [
        "replay_buffer = ReplayBuffer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "dhC_5XJ__Orp",
        "outputId": "ebd75603-cf61-4732-aa86-da701def1d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.807990\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MTL9uMd0ru03"
      },
      "outputs": [],
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1vN5EvxK_QhT"
      },
      "outputs": [],
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y_ouY4NH_Y0I",
        "outputId": "01b4c620-92d9-4cc7-f11d-ac8cc11dfdb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Timesteps: 98 Episode Num: 1 Reward: 47.215498117369116\n",
            "Total Timesteps: 1098 Episode Num: 2 Reward: 493.55674622756\n",
            "Total Timesteps: 1672 Episode Num: 3 Reward: 308.83571209444887\n",
            "Total Timesteps: 2672 Episode Num: 4 Reward: 491.68335593148345\n",
            "Total Timesteps: 3672 Episode Num: 5 Reward: 517.6874423675083\n",
            "Total Timesteps: 4672 Episode Num: 6 Reward: 490.75244250818093\n",
            "Total Timesteps: 4862 Episode Num: 7 Reward: 96.17598261059179\n",
            "Total Timesteps: 5242 Episode Num: 8 Reward: 178.5699111924712\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 137.016474\n",
            "---------------------------------------\n",
            "Total Timesteps: 6242 Episode Num: 9 Reward: 475.67871872246127\n",
            "Total Timesteps: 7242 Episode Num: 10 Reward: 518.3955296461515\n",
            "Total Timesteps: 8242 Episode Num: 11 Reward: 484.7690794131638\n",
            "Total Timesteps: 9242 Episode Num: 12 Reward: 500.286543167322\n",
            "Total Timesteps: 10242 Episode Num: 13 Reward: 429.59927681325735\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 159.254701\n",
            "---------------------------------------\n",
            "Total Timesteps: 11242 Episode Num: 14 Reward: 128.51828677226305\n",
            "Total Timesteps: 11483 Episode Num: 15 Reward: 34.70361312920723\n",
            "Total Timesteps: 12178 Episode Num: 16 Reward: 186.06614545477322\n",
            "Total Timesteps: 13178 Episode Num: 17 Reward: 222.94380369330207\n",
            "Total Timesteps: 14178 Episode Num: 18 Reward: 292.4244728302809\n",
            "Total Timesteps: 15178 Episode Num: 19 Reward: 168.4487450704216\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 6.078016\n",
            "---------------------------------------\n",
            "Total Timesteps: 15205 Episode Num: 20 Reward: 4.9383147898397315\n",
            "Total Timesteps: 15232 Episode Num: 21 Reward: 6.1252621633310005\n",
            "Total Timesteps: 15260 Episode Num: 22 Reward: 6.373700975052049\n",
            "Total Timesteps: 15287 Episode Num: 23 Reward: 4.786571967522235\n",
            "Total Timesteps: 15314 Episode Num: 24 Reward: 4.96881762408594\n",
            "Total Timesteps: 15342 Episode Num: 25 Reward: 8.988015500641161\n",
            "Total Timesteps: 15370 Episode Num: 26 Reward: 10.448553152719127\n",
            "Total Timesteps: 15398 Episode Num: 27 Reward: 9.059679505621085\n",
            "Total Timesteps: 15425 Episode Num: 28 Reward: 6.024557972683143\n",
            "Total Timesteps: 15447 Episode Num: 29 Reward: -4.63585273981049\n",
            "Total Timesteps: 15474 Episode Num: 30 Reward: 9.086425840479892\n",
            "Total Timesteps: 15502 Episode Num: 31 Reward: 7.205977100147866\n",
            "Total Timesteps: 15528 Episode Num: 32 Reward: 6.312644954616717\n",
            "Total Timesteps: 15553 Episode Num: 33 Reward: 1.7279029891646065\n",
            "Total Timesteps: 15578 Episode Num: 34 Reward: 2.0431099775440904\n",
            "Total Timesteps: 15604 Episode Num: 35 Reward: 5.259770556161525\n",
            "Total Timesteps: 15631 Episode Num: 36 Reward: 7.621088141750943\n",
            "Total Timesteps: 15654 Episode Num: 37 Reward: -0.7893208587241811\n",
            "Total Timesteps: 15676 Episode Num: 38 Reward: -2.9924910367085453\n",
            "Total Timesteps: 15702 Episode Num: 39 Reward: 7.084350407487208\n",
            "Total Timesteps: 15729 Episode Num: 40 Reward: 9.961274203607681\n",
            "Total Timesteps: 15753 Episode Num: 41 Reward: -1.517672364152007\n",
            "Total Timesteps: 15782 Episode Num: 42 Reward: 14.38892493718134\n",
            "Total Timesteps: 15808 Episode Num: 43 Reward: 7.7338925946430885\n",
            "Total Timesteps: 15835 Episode Num: 44 Reward: 6.679457700657885\n",
            "Total Timesteps: 15857 Episode Num: 45 Reward: -2.041257502608402\n",
            "Total Timesteps: 15879 Episode Num: 46 Reward: -2.3821684173076867\n",
            "Total Timesteps: 15903 Episode Num: 47 Reward: 0.09351464035275825\n",
            "Total Timesteps: 15927 Episode Num: 48 Reward: 1.675479664786474\n",
            "Total Timesteps: 15954 Episode Num: 49 Reward: 9.400384289506537\n",
            "Total Timesteps: 15981 Episode Num: 50 Reward: 4.183054061761569\n",
            "Total Timesteps: 16005 Episode Num: 51 Reward: 0.5391783711234903\n",
            "Total Timesteps: 16031 Episode Num: 52 Reward: 3.9431423232194835\n",
            "Total Timesteps: 16055 Episode Num: 53 Reward: -1.0862716421133696\n",
            "Total Timesteps: 16168 Episode Num: 54 Reward: 16.339613954677446\n",
            "Total Timesteps: 16196 Episode Num: 55 Reward: 7.101196174579217\n",
            "Total Timesteps: 16222 Episode Num: 56 Reward: 7.587221653113978\n",
            "Total Timesteps: 16248 Episode Num: 57 Reward: 4.273353396493893\n",
            "Total Timesteps: 16276 Episode Num: 58 Reward: 10.953651589324215\n",
            "Total Timesteps: 16301 Episode Num: 59 Reward: 1.9461502066118372\n",
            "Total Timesteps: 16327 Episode Num: 60 Reward: 1.3528792733441566\n",
            "Total Timesteps: 16355 Episode Num: 61 Reward: 9.677707505417045\n",
            "Total Timesteps: 16384 Episode Num: 62 Reward: 11.765576203674286\n",
            "Total Timesteps: 16413 Episode Num: 63 Reward: 10.683088616592261\n",
            "Total Timesteps: 16440 Episode Num: 64 Reward: 7.820730166821219\n",
            "Total Timesteps: 16466 Episode Num: 65 Reward: 8.41402227902509\n",
            "Total Timesteps: 16494 Episode Num: 66 Reward: 7.92301780618976\n",
            "Total Timesteps: 16523 Episode Num: 67 Reward: 10.708364517502924\n",
            "Total Timesteps: 16551 Episode Num: 68 Reward: 9.726768285553385\n",
            "Total Timesteps: 16577 Episode Num: 69 Reward: 3.593582266403403\n",
            "Total Timesteps: 16602 Episode Num: 70 Reward: 1.0501059793101741\n",
            "Total Timesteps: 16646 Episode Num: 71 Reward: -0.20100326735149343\n",
            "Total Timesteps: 16675 Episode Num: 72 Reward: 11.348191815101385\n",
            "Total Timesteps: 16704 Episode Num: 73 Reward: 8.340138113789491\n",
            "Total Timesteps: 16775 Episode Num: 74 Reward: 10.419241291323821\n",
            "Total Timesteps: 16798 Episode Num: 75 Reward: -1.114223781459546\n",
            "Total Timesteps: 16821 Episode Num: 76 Reward: -2.198720533165309\n",
            "Total Timesteps: 16850 Episode Num: 77 Reward: 9.177848290486951\n",
            "Total Timesteps: 16878 Episode Num: 78 Reward: 7.69982156888995\n",
            "Total Timesteps: 16905 Episode Num: 79 Reward: 8.767030670213533\n",
            "Total Timesteps: 16934 Episode Num: 80 Reward: 10.814943376681057\n",
            "Total Timesteps: 16960 Episode Num: 81 Reward: 3.889201714481631\n",
            "Total Timesteps: 16988 Episode Num: 82 Reward: 6.692220379117397\n",
            "Total Timesteps: 17015 Episode Num: 83 Reward: 4.475709496595433\n",
            "Total Timesteps: 17083 Episode Num: 84 Reward: 12.712058159805876\n",
            "Total Timesteps: 17110 Episode Num: 85 Reward: 8.122321358706458\n",
            "Total Timesteps: 17139 Episode Num: 86 Reward: 6.196766821907838\n",
            "Total Timesteps: 17165 Episode Num: 87 Reward: 2.3482518592625037\n",
            "Total Timesteps: 17187 Episode Num: 88 Reward: -1.5734276965016591\n",
            "Total Timesteps: 17211 Episode Num: 89 Reward: -1.475874650731405\n",
            "Total Timesteps: 17235 Episode Num: 90 Reward: 0.25530445342766894\n",
            "Total Timesteps: 17263 Episode Num: 91 Reward: 8.573342961391234\n",
            "Total Timesteps: 17291 Episode Num: 92 Reward: 7.891254719431349\n",
            "Total Timesteps: 17352 Episode Num: 93 Reward: 10.168004357659848\n",
            "Total Timesteps: 17422 Episode Num: 94 Reward: 1.7291629475333916\n",
            "Total Timesteps: 17446 Episode Num: 95 Reward: -0.9031100992564793\n",
            "Total Timesteps: 17473 Episode Num: 96 Reward: 7.169351594046413\n",
            "Total Timesteps: 17500 Episode Num: 97 Reward: 3.934795646574437\n",
            "Total Timesteps: 17528 Episode Num: 98 Reward: 6.568040506816693\n",
            "Total Timesteps: 17554 Episode Num: 99 Reward: 4.358573121443451\n",
            "Total Timesteps: 17592 Episode Num: 100 Reward: 12.722205933465332\n",
            "Total Timesteps: 17621 Episode Num: 101 Reward: 9.05354191970113\n",
            "Total Timesteps: 17649 Episode Num: 102 Reward: 2.1111918471020434\n",
            "Total Timesteps: 17672 Episode Num: 103 Reward: -0.4785439456003826\n",
            "Total Timesteps: 17699 Episode Num: 104 Reward: 6.022708456313371\n",
            "Total Timesteps: 17727 Episode Num: 105 Reward: 7.73075438459363\n",
            "Total Timesteps: 17814 Episode Num: 106 Reward: 9.685897208712694\n",
            "Total Timesteps: 17839 Episode Num: 107 Reward: -0.02039782072376317\n",
            "Total Timesteps: 17865 Episode Num: 108 Reward: 3.4217993000255156\n",
            "Total Timesteps: 17888 Episode Num: 109 Reward: -1.1364008684911262\n",
            "Total Timesteps: 17914 Episode Num: 110 Reward: 3.1652722118770744\n",
            "Total Timesteps: 17939 Episode Num: 111 Reward: 1.1802710601027053\n",
            "Total Timesteps: 17963 Episode Num: 112 Reward: -2.0378421951562937\n",
            "Total Timesteps: 17991 Episode Num: 113 Reward: 8.513232273938186\n",
            "Total Timesteps: 18018 Episode Num: 114 Reward: 3.4359862718115917\n",
            "Total Timesteps: 18042 Episode Num: 115 Reward: 0.29741916947310765\n",
            "Total Timesteps: 18067 Episode Num: 116 Reward: 2.133445871236728\n",
            "Total Timesteps: 18093 Episode Num: 117 Reward: 1.8704607679252614\n",
            "Total Timesteps: 18119 Episode Num: 118 Reward: 2.8894270093340144\n",
            "Total Timesteps: 18145 Episode Num: 119 Reward: 3.325743510344901\n",
            "Total Timesteps: 18168 Episode Num: 120 Reward: -1.4995110778891667\n",
            "Total Timesteps: 18232 Episode Num: 121 Reward: 4.939992002928311\n",
            "Total Timesteps: 18255 Episode Num: 122 Reward: -2.0425006328517856\n",
            "Total Timesteps: 18299 Episode Num: 123 Reward: 0.49777216183124384\n",
            "Total Timesteps: 18323 Episode Num: 124 Reward: 0.6119822686586989\n",
            "Total Timesteps: 18347 Episode Num: 125 Reward: -0.26813463336180865\n",
            "Total Timesteps: 18374 Episode Num: 126 Reward: 5.869538102175776\n",
            "Total Timesteps: 18443 Episode Num: 127 Reward: 3.6106984195541947\n",
            "Total Timesteps: 18471 Episode Num: 128 Reward: 9.985573161953935\n",
            "Total Timesteps: 18500 Episode Num: 129 Reward: 10.36739101578427\n",
            "Total Timesteps: 18527 Episode Num: 130 Reward: 6.771729942968733\n",
            "Total Timesteps: 18553 Episode Num: 131 Reward: 1.4324983244039144\n",
            "Total Timesteps: 18601 Episode Num: 132 Reward: 7.801868627719447\n",
            "Total Timesteps: 18628 Episode Num: 133 Reward: 7.972693681870457\n",
            "Total Timesteps: 18686 Episode Num: 134 Reward: 11.04182883561356\n",
            "Total Timesteps: 18710 Episode Num: 135 Reward: -0.633794049100299\n",
            "Total Timesteps: 18735 Episode Num: 136 Reward: 1.470876955274346\n",
            "Total Timesteps: 18764 Episode Num: 137 Reward: -0.2920452576718222\n",
            "Total Timesteps: 18822 Episode Num: 138 Reward: 9.744877582660639\n",
            "Total Timesteps: 18850 Episode Num: 139 Reward: 4.649310863882982\n",
            "Total Timesteps: 18877 Episode Num: 140 Reward: 4.711761866790303\n",
            "Total Timesteps: 18900 Episode Num: 141 Reward: 1.5142549004667982\n",
            "Total Timesteps: 18927 Episode Num: 142 Reward: 5.042058539300257\n",
            "Total Timesteps: 18951 Episode Num: 143 Reward: 0.2338875468581576\n",
            "Total Timesteps: 18978 Episode Num: 144 Reward: 3.6308490078044664\n",
            "Total Timesteps: 19005 Episode Num: 145 Reward: 3.9132187714170765\n",
            "Total Timesteps: 19034 Episode Num: 146 Reward: 8.647952094897908\n",
            "Total Timesteps: 19061 Episode Num: 147 Reward: 4.58936123420523\n",
            "Total Timesteps: 19101 Episode Num: 148 Reward: 11.866913137459237\n",
            "Total Timesteps: 19188 Episode Num: 149 Reward: 19.8268752628981\n",
            "Total Timesteps: 19420 Episode Num: 150 Reward: 57.92397137612332\n",
            "Total Timesteps: 19450 Episode Num: 151 Reward: 9.221694402994789\n",
            "Total Timesteps: 19489 Episode Num: 152 Reward: 13.613385483623798\n",
            "Total Timesteps: 19519 Episode Num: 153 Reward: 8.318095812103785\n",
            "Total Timesteps: 19551 Episode Num: 154 Reward: 14.324934064685367\n",
            "Total Timesteps: 19692 Episode Num: 155 Reward: 42.465417776789884\n",
            "Total Timesteps: 20692 Episode Num: 156 Reward: 186.6200221954093\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 229.880671\n",
            "---------------------------------------\n",
            "Total Timesteps: 21434 Episode Num: 157 Reward: 163.22533022386648\n",
            "Total Timesteps: 22434 Episode Num: 158 Reward: 114.82254587422165\n",
            "Total Timesteps: 23434 Episode Num: 159 Reward: 448.98754764901093\n",
            "Total Timesteps: 24434 Episode Num: 160 Reward: 396.085541287488\n",
            "Total Timesteps: 25434 Episode Num: 161 Reward: 302.28766960199664\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 291.281509\n",
            "---------------------------------------\n",
            "Total Timesteps: 26434 Episode Num: 162 Reward: 297.59602184970413\n",
            "Total Timesteps: 27434 Episode Num: 163 Reward: 311.89993055730315\n",
            "Total Timesteps: 28434 Episode Num: 164 Reward: 304.0125719826845\n",
            "Total Timesteps: 29434 Episode Num: 165 Reward: 292.0636949164049\n",
            "Total Timesteps: 30434 Episode Num: 166 Reward: 322.44105186210385\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 527.114205\n",
            "---------------------------------------\n",
            "Total Timesteps: 31434 Episode Num: 167 Reward: 474.08450988509844\n",
            "Total Timesteps: 32434 Episode Num: 168 Reward: 336.515629490217\n",
            "Total Timesteps: 33434 Episode Num: 169 Reward: 256.3909738956988\n",
            "Total Timesteps: 33576 Episode Num: 170 Reward: 75.80036156417844\n",
            "Total Timesteps: 33734 Episode Num: 171 Reward: 73.69995047935565\n",
            "Total Timesteps: 34734 Episode Num: 172 Reward: 398.5861086535666\n",
            "Total Timesteps: 35734 Episode Num: 173 Reward: 216.71728923455885\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 329.996124\n",
            "---------------------------------------\n",
            "Total Timesteps: 36734 Episode Num: 174 Reward: 239.6101418602652\n",
            "Total Timesteps: 37734 Episode Num: 175 Reward: 282.94413820251856\n",
            "Total Timesteps: 38734 Episode Num: 176 Reward: 198.29095761052858\n",
            "Total Timesteps: 39734 Episode Num: 177 Reward: 428.4978865218456\n",
            "Total Timesteps: 40734 Episode Num: 178 Reward: 118.21730545159633\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 143.051510\n",
            "---------------------------------------\n",
            "Total Timesteps: 41734 Episode Num: 179 Reward: 201.01535090155664\n",
            "Total Timesteps: 41754 Episode Num: 180 Reward: 3.139573311063645\n",
            "Total Timesteps: 41774 Episode Num: 181 Reward: 2.3436436025730183\n",
            "Total Timesteps: 41797 Episode Num: 182 Reward: 2.7294868489861646\n",
            "Total Timesteps: 41824 Episode Num: 183 Reward: 7.859241140813142\n",
            "Total Timesteps: 41856 Episode Num: 184 Reward: 5.472476104838172\n",
            "Total Timesteps: 41876 Episode Num: 185 Reward: 3.276736108358131\n",
            "Total Timesteps: 41896 Episode Num: 186 Reward: 3.3373961986933054\n",
            "Total Timesteps: 42896 Episode Num: 187 Reward: 328.8230023578782\n",
            "Total Timesteps: 43896 Episode Num: 188 Reward: 466.53330253611176\n",
            "Total Timesteps: 44896 Episode Num: 189 Reward: 581.9119342543141\n",
            "Total Timesteps: 45896 Episode Num: 190 Reward: 342.62026448074516\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 34.375198\n",
            "---------------------------------------\n",
            "Total Timesteps: 46896 Episode Num: 191 Reward: 603.8651896813219\n",
            "Total Timesteps: 47896 Episode Num: 192 Reward: 307.3528759265949\n",
            "Total Timesteps: 48421 Episode Num: 193 Reward: 283.79332958024014\n",
            "Total Timesteps: 49421 Episode Num: 194 Reward: 305.1163083540745\n",
            "Total Timesteps: 50421 Episode Num: 195 Reward: 281.42247012875845\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 276.033798\n",
            "---------------------------------------\n",
            "Total Timesteps: 50676 Episode Num: 196 Reward: 36.447027999838234\n",
            "Total Timesteps: 51676 Episode Num: 197 Reward: 427.07723807629884\n",
            "Total Timesteps: 52676 Episode Num: 198 Reward: 565.8224282437848\n",
            "Total Timesteps: 53080 Episode Num: 199 Reward: 200.10712319742203\n",
            "Total Timesteps: 54080 Episode Num: 200 Reward: 372.34397462618784\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\navya\\OneDrive\\Desktop\\ERV1\\ERAV1-Session-25-main\\ERA1S25.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m total_timesteps \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTotal Timesteps: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m Episode Num: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m Reward: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(total_timesteps, episode_num, episode_reward))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   policy\u001b[39m.\u001b[39mtrain(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# We evaluate the episode and we save the policy\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m timesteps_since_eval \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m eval_freq:\n",
            "\u001b[1;32mc:\\Users\\navya\\OneDrive\\Desktop\\ERV1\\ERAV1-Session-25-main\\ERA1S25.ipynb Cell 37\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mif\u001b[39;00m it \u001b[39m%\u001b[39m policy_freq \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m   actor_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mQ1(state, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor(state))\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m   actor_loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\navya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32mc:\\Users\\navya\\OneDrive\\Desktop\\ERV1\\ERAV1-Session-25-main\\ERA1S25.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_2(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navya/OneDrive/Desktop/ERV1/ERAV1-Session-25-main/ERA1S25.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_action \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_3(x))\n",
            "File \u001b[1;32mc:\\Users\\navya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\navya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "max_timesteps = 500000\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "\n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "\n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "\n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "\n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "\n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "\n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "\n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "\n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcnexWrW4a8P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
